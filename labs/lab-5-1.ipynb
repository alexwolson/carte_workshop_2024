{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uO72w9ylnZxk"
   },
   "source": [
    "# Face verification using Siamese Networks\n",
    "\n",
    "In this lab, we will look at a simple face verification system using a Siamese network. We will use the LFW dataset, which is a collection of face images of famous people. We will train a Siamese network to recognize whether two images belong to the same person or not.\n",
    "\n",
    "### Goals\n",
    "- train a network for face similarity using siamese networks\n",
    "- work data augmentation, generators and hard negative mining\n",
    "- use the model on your picture (if you like)\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- We will be using Labeled Faces in the Wild (LFW) dataset available openly at [http://vis-www.cs.umass.edu/lfw/](http://vis-www.cs.umass.edu/lfw/).\n",
    "- For computing purposes, we'll only restrict ourselves to a subpart of the dataset. You're welcome to train on the whole dataset on GPU, by setting `USE_SUBSET=False` in the following cells,\n",
    "- We will also load pretrained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "from urllib.request import urlretrieve\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "URL = \"http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz\"\n",
    "FILENAME = \"lfw-deepfunneled.tgz\"\n",
    "\n",
    "def reporthook(blocknum, blocksize, totalsize):\n",
    "    readsofar = blocknum * blocksize\n",
    "    if totalsize > 0:\n",
    "        percent = readsofar * 1e2 / totalsize\n",
    "        bar_length = 50\n",
    "        filled_length = int(percent * bar_length / 100)\n",
    "        bar = '=' * filled_length + '-' * (bar_length - filled_length)\n",
    "        print(f\"\\rDownloading: [{bar}] {percent:.1f}%\", end=\"\")\n",
    "    else:\n",
    "        print(\"read %d\\n\" % readsofar, end=\"\")\n",
    "\n",
    "if not op.exists(FILENAME):\n",
    "    print('Downloading %s to %s...' % (URL, FILENAME))\n",
    "    urlretrieve(URL, FILENAME, reporthook=reporthook)\n",
    "\n",
    "if not op.exists(\"lfw\"):\n",
    "    print('\\nExtracting image files...')\n",
    "    with tarfile.open(\"lfw-deepfunneled.tgz\") as tar:\n",
    "        num_members = len(list(tar.getmembers()))  # Get total for basic progress\n",
    "        extract_count = 0\n",
    "        for member in tar.getmembers():\n",
    "            tar.extract(member, \"lfw\")\n",
    "            extract_count += 1\n",
    "            percent = extract_count * 100 / num_members\n",
    "            print(f\"\\rExtracting: {percent:.1f}%\", end=\"\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfgOqzIcnZxm",
    "outputId": "0582d8d1-93ca-4115-a4d4-b5138c9f8ce4"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_oZ73-InZxn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, GlobalAveragePooling2D, Dropout\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgKWcF8dnZxo"
   },
   "source": [
    "## Processing the dataset\n",
    "\n",
    "The dataset consists of folders corresponding to each identity. The folder name is the name of the person.\n",
    "We map each class (identity) to an integer id, and build mappings as dictionaries `name_to_classid` and `classid_to_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB7cnbhhnZxp"
   },
   "outputs": [],
   "source": [
    "PATH = \"lfw/lfw-deepfunneled/\"\n",
    "USE_SUBSET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "STlniCzmnZxp",
    "outputId": "6b4c7ca1-c9f3-4702-fee3-8759812198b8"
   },
   "outputs": [],
   "source": [
    "dirs = sorted(os.listdir(PATH))\n",
    "if USE_SUBSET:\n",
    "    dirs = dirs[:500]\n",
    "\n",
    "name_to_classid = {d: i for i, d in enumerate(dirs)}\n",
    "classid_to_name = {v: k for k, v in name_to_classid.items()}\n",
    "num_classes = len(name_to_classid)\n",
    "\n",
    "print(f'Number of classes (i.e. people): {num_classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-e4BFe4nZxq"
   },
   "source": [
    "In each directory, there is one or more images corresponding to the identity. We map each image path with an integer id, then build a few dictionaries:\n",
    "- mappings from imagepath and image id: `path_to_id` and `id_to_path`\n",
    "- mappings from class id to image ids: `classid_to_ids` and `id_to_classid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Kgh-115nZxr"
   },
   "outputs": [],
   "source": [
    "# read all directories\n",
    "img_paths = {c: [PATH + subfolder + \"/\" + img\n",
    "                 for img in sorted(os.listdir(PATH + subfolder))]\n",
    "             for subfolder, c in name_to_classid.items()}\n",
    "\n",
    "# retrieve all images\n",
    "all_images_path = []\n",
    "for img_list in img_paths.values():\n",
    "    all_images_path += img_list\n",
    "\n",
    "# map to integers\n",
    "path_to_id = {v: k for k, v in enumerate(all_images_path)}\n",
    "id_to_path = {v: k for k, v in path_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSSEacdynZxs",
    "outputId": "740ed1e2-af3a-4848-eca8-1b3c76bd6d95"
   },
   "outputs": [],
   "source": [
    "all_images_path[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Q8gsK3QnZxt",
    "outputId": "c3d62f84-37ce-45eb-a924-96e09aa3441d"
   },
   "outputs": [],
   "source": [
    "len(all_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_YqTK1nnZxt",
    "outputId": "12e10283-640d-4f87-bcdc-186a7a6281eb"
   },
   "outputs": [],
   "source": [
    "# build mappings between images and class\n",
    "classid_to_ids = {k: [path_to_id[path] for path in v] for k, v in img_paths.items()}\n",
    "id_to_classid = {v: c for c, imgs in classid_to_ids.items() for v in imgs}\n",
    "dict(list(id_to_classid.items())[0:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZBO3mgSnZxu"
   },
   "source": [
    "The following histogram shows the number of images per class: there are many classes with only one image. Since we need at least two examples of an identity to build a positive pair, these classes will only be used for negative pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "tQwvbcR4nZxu",
    "outputId": "3a53c286-9f9e-47e2-a0fe-224fa0bd926a"
   },
   "outputs": [],
   "source": [
    "plt.hist([len(v) for k, v in classid_to_ids.items()], bins=range(1, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ar_ZfE7TnZxv",
    "outputId": "9bb47998-8b0f-4c38-f21a-0d78860692fb"
   },
   "outputs": [],
   "source": [
    "np.median([len(ids) for ids in classid_to_ids.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NzJWjzvcnZxv",
    "outputId": "24556e4e-f256-4d5d-bda0-65f9fc5f3102"
   },
   "outputs": [],
   "source": [
    "[(classid_to_name[x], len(classid_to_ids[x]))\n",
    " for x in np.argsort([len(v) for k, v in classid_to_ids.items()])[::-1][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display some images\n",
    "from random import choice\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    class_id = choice(list(classid_to_ids.keys()))\n",
    "    img_id = choice(classid_to_ids[class_id])\n",
    "    img = plt.imread(id_to_path[img_id])\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(classid_to_name[class_id])\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "id": "uRcK7fCwnZxv",
    "outputId": "cf6b2180-4267-41b5-fff4-e2aedc866a62"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXSSaAJhnZxw"
   },
   "source": [
    "### Siamese nets\n",
    "\n",
    "A siamese net takes as input two images $x_1$ and $x_2$ and outputs a single value which corresponds to the similarity between $x_1$ and $x_2$.\n",
    "\n",
    "\n",
    "\n",
    "In order to train such a system, we need to create pairs of positive examples and negative examples. We will use the following two methods to build the pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqmHmtWknZxw"
   },
   "outputs": [],
   "source": [
    "# Build pairs of positive image ids for a given classid\n",
    "def build_pos_pairs_for_id(classid, max_num=70):\n",
    "    imgs = classid_to_ids[classid]\n",
    "\n",
    "    if len(imgs) == 1:\n",
    "        return []\n",
    "\n",
    "    pos_pairs = list(itertools.combinations(imgs, 2))\n",
    "\n",
    "    random.shuffle(pos_pairs)\n",
    "    return pos_pairs[:max_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEav5Oz1nZxw"
   },
   "outputs": [],
   "source": [
    "# Build pairs of negative image ids for a given classid\n",
    "def build_neg_pairs_for_id(classid, classes, max_num=3):\n",
    "    imgs = classid_to_ids[classid]\n",
    "    neg_classes_ids = random.sample(classes, max_num+1)\n",
    "\n",
    "    if classid in neg_classes_ids:\n",
    "        neg_classes_ids.remove(classid)\n",
    "\n",
    "    neg_pairs = []\n",
    "    for id2 in range(max_num):\n",
    "        img1 = imgs[random.randint(0, len(imgs) - 1)]\n",
    "        imgs2 = classid_to_ids[neg_classes_ids[id2]]\n",
    "        img2 = imgs2[random.randint(0, len(imgs2) - 1)]\n",
    "        neg_pairs += [(img1, img2)]\n",
    "\n",
    "    return neg_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vplilUBnnZxx"
   },
   "source": [
    "Let's build positive and a negative pairs for class 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZNkNMxrnZxx",
    "outputId": "078abfad-19ff-4807-9477-aa897b96db31"
   },
   "outputs": [],
   "source": [
    "build_pos_pairs_for_id(5, max_num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Z1tW9KVnZxx",
    "outputId": "e6b7c078-d140-42cc-fe4d-c60a62d0b562"
   },
   "outputs": [],
   "source": [
    "build_neg_pairs_for_id(5, list(range(num_classes)), max_num=6)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Visualize a positive pair and a negative pair\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "img1, img2 = build_pos_pairs_for_id(5, max_num=1)[0]\n",
    "\n",
    "ax[0, 0].imshow(plt.imread(id_to_path[img1]))\n",
    "ax[0, 0].set_title(\"positive pair\")\n",
    "ax[0, 0].axis('off')\n",
    "\n",
    "ax[0, 1].imshow(plt.imread(id_to_path[img2]))\n",
    "ax[0, 1].axis('off')\n",
    "\n",
    "img1, img2 = build_neg_pairs_for_id(5, list(range(num_classes)), max_num=1)[0]\n",
    "\n",
    "ax[1, 0].imshow(plt.imread(id_to_path[img1]))\n",
    "ax[1, 0].set_title(\"negative pair\")\n",
    "ax[1, 0].axis('off')\n",
    "\n",
    "ax[1, 1].imshow(plt.imread(id_to_path[img2]))\n",
    "ax[1, 1].axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "id": "gE_8n85FnZxy",
    "outputId": "641935db-c8cd-4e7c-9af1-adfdc7ecaef0"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qame8AhEnZxy"
   },
   "source": [
    "Now that we have a way to compute the pairs, let's load all the possible JPEG-compressed image files into a single numpy array in RAM. There are around 1000 images, so 100MB of RAM will be used, which should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IpJYZx4nZxy"
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "def resize100(img): # resize to 100x100\n",
    "    return resize(\n",
    "        img, (100, 100), preserve_range=True, mode='reflect', anti_aliasing=True\n",
    "    )[20:80, 20:80, :]\n",
    "\n",
    "\n",
    "def open_all_images(id_to_path):\n",
    "    all_imgs = []\n",
    "    for path in id_to_path.values():\n",
    "        all_imgs += [np.expand_dims(resize100(imread(path)), 0)]\n",
    "    return np.vstack(all_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMw0a4vVnZxy",
    "outputId": "1a2b51ec-d736-469f-c7c0-fb3166f409be"
   },
   "outputs": [],
   "source": [
    "all_imgs = open_all_images(id_to_path)\n",
    "all_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4ebj2vanZxz",
    "outputId": "9240f232-1f80-41d2-c9cd-0c4ef0b1809e"
   },
   "outputs": [],
   "source": [
    "print(f\"{all_imgs.nbytes / 1e6} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBpIMeD5nZxz"
   },
   "source": [
    "The following function builds a large number of positives/negatives pairs (train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEod8FPnnZxz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data_pairs():\n",
    "    \"\"\"Generates (x1, x2, y) data pairs for all classes.\"\"\"\n",
    "\n",
    "    all_input_1 = []\n",
    "    all_input_2 = []\n",
    "    all_labels = []\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        positive_pairs = build_pos_pairs_for_id(class_id)\n",
    "        negative_pairs = build_neg_pairs_for_id(class_id, list(range(num_classes)))\n",
    "\n",
    "        for pair in positive_pairs:\n",
    "            all_input_1.append(pair[0])\n",
    "            all_input_2.append(pair[1])\n",
    "            all_labels.append(1)\n",
    "\n",
    "        for pair in negative_pairs:\n",
    "            all_input_1.append(pair[0])\n",
    "            all_input_2.append(pair[1])\n",
    "            all_labels.append(0)\n",
    "\n",
    "    return np.array(all_input_1), np.array(all_input_2), np.array(all_labels)\n",
    "\n",
    "def split_into_train_test(X1, X2, Y, train_data_ratio=0.8):\n",
    "    \"\"\"Splits data into training and testing sets.\"\"\"\n",
    "\n",
    "    X1_train, X1_test, X2_train, X2_test, Y_train, Y_test = train_test_split(\n",
    "        X1, X2, Y, test_size=1 - train_data_ratio\n",
    "    )\n",
    "    return X1_train, X2_train, Y_train, X1_test, X2_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGABj1udnZxz"
   },
   "outputs": [],
   "source": [
    "X1, X2, Y = generate_data_pairs()\n",
    "X1_train, X2_train, Y_train, X1_test, X2_test, Y_test = split_into_train_test(X1, X2, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1se9OgInZxz",
    "outputId": "ccc788ae-9705-4a5e-9e12-c5190ce5dab6"
   },
   "outputs": [],
   "source": [
    "X1_train.shape, X2_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "idVJBQxZnZx0",
    "outputId": "309ee706-6a6f-43cb-e3cd-03813924ac72"
   },
   "outputs": [],
   "source": [
    "np.mean(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8YaC529nZx0",
    "outputId": "56bf417d-afe1-4953-d470-3cd5cf00b2ab"
   },
   "outputs": [],
   "source": [
    "X1_test.shape, X2_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kbD_j-B8nZx0",
    "outputId": "356a9ce0-15f7-4ddc-e714-bc3119f2cb1d"
   },
   "outputs": [],
   "source": [
    "np.mean(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJtYH846nZx1"
   },
   "source": [
    "**Data augmentation and generator**\n",
    "\n",
    "We're building a generator, which will modify images through data augmentation on the fly. This is useful when we have a large dataset and we don't want to store all the augmented images in memory. We will use the `ImageDataGenerator` from Keras to perform the augmentation. We will use the generator to train the model.\n",
    "\n",
    "You can add more image augmentations to the generator, such as rotation, zooming, etc. The goal is to make the model more robust to different lighting conditions, rotations, etc. Have a look at the documentation [here](https://keras.io/preprocessing/image/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dD45Bd8YnZx1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "class Generator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, X1, X2, Y, batch_size, all_imgs):\n",
    "        self.batch_size = batch_size\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "        self.Y = Y\n",
    "        self.imgs = all_imgs\n",
    "        self.num_samples = Y.shape[0]\n",
    "\n",
    "        # Create ImageDataGenerator for augmentation\n",
    "        self.image_datagen = ImageDataGenerator(\n",
    "            horizontal_flip=True  # Replace with other desired augmentations\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples // self.batch_size\n",
    "\n",
    "    def __getitem__(self, batch_index):\n",
    "        \"\"\"This method returns the `batch_index`-th batch of the dataset.\"\"\"\n",
    "\n",
    "        low_index = batch_index * self.batch_size\n",
    "        high_index = (batch_index + 1) * self.batch_size\n",
    "\n",
    "        # Fetch image data\n",
    "        imgs1 = self.imgs[self.X1[low_index:high_index]]\n",
    "        imgs2 = self.imgs[self.X2[low_index:high_index]]\n",
    "        targets = self.Y[low_index:high_index]\n",
    "\n",
    "        # Apply augmentation in batches\n",
    "        imgs1 = self.image_datagen.flow(imgs1, batch_size=self.batch_size, shuffle=False).next()\n",
    "        imgs2 = self.image_datagen.flow(imgs2, batch_size=self.batch_size, shuffle=False).next()\n",
    "\n",
    "        return ([imgs1, imgs2], targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxPcjW-NnZx1"
   },
   "outputs": [],
   "source": [
    "gen = Generator(X1_train, X2_train, Y_train, 32, all_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlypx0QgnZx1",
    "outputId": "4f3d20b0-7189-409a-ccfd-1296d98b2c67"
   },
   "outputs": [],
   "source": [
    "print(\"Number of batches: {}\".format(len(gen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L11OJl3EnZx2",
    "outputId": "2a3d0af8-8b41-4e81-c8f0-cc93428f2413"
   },
   "outputs": [],
   "source": [
    "[x1, x2], y = gen[0]\n",
    "\n",
    "x1.shape, x2.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "o5pAnn_enZx2",
    "outputId": "85df8ece-e7e0-42d5-a9f9-efce8eee57f4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 6, i + 1)\n",
    "    plt.imshow(x1[i] / 255)\n",
    "    plt.axis('off')\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 6, i + 7)\n",
    "    plt.imshow(x2[i] / 255)\n",
    "    if y[i]==1.0:\n",
    "        plt.title(\"similar\")\n",
    "    else:\n",
    "        plt.title(\"different\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSfi1dNLnZx3"
   },
   "source": [
    "**Exercise**\n",
    "- Add your own data augmentations in the process.\n",
    "- Be careful not to make the task to difficult, and to add meaningful augmentations;\n",
    "- Re-run the generator plot above to check whether the image pairs look not too distorted to recognize the identities.\n",
    "\n",
    "** Test images **\n",
    "\n",
    "- In addition to our generator, we need test images, unaffected by the augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3uyhAMZnZx3",
    "outputId": "91a050d4-f873-4ca4-a6a9-d9579baf8332"
   },
   "outputs": [],
   "source": [
    "test_X1 = all_imgs[X1_test]\n",
    "test_X2 = all_imgs[X2_test]\n",
    "\n",
    "test_X1.shape, test_X2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k92jzTkjnZx3"
   },
   "source": [
    "## Simple convolutional model\n",
    "\n",
    "We will build a simple convolutional model which takes an image and outputs a vector of a fixed dimension. We will use this model as a shared model for the two inputs of the siamese network. For the network, we need to define a custom loss function (included in the model definition) and a custom accuracy function, which we will use to monitor the training process."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def accuracy_sim(y_true, y_pred, threshold=0.5):\n",
    "    y_thresholded = tf.cast(y_pred > threshold, \"float32\")\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_true, y_thresholded), \"float32\"))"
   ],
   "metadata": {
    "id": "oivbRby3nZx4"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuZBcP6rnZx4"
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "Complete the following Siamese model. The model should take two inputs, run them through the shared convolutional model, and then compute the similarity between the two vectors using a custom layer.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evNiWH0OnZx4"
   },
   "outputs": [],
   "source": [
    "def build_shared_conv(input_shape=(60, 60, 3)):\n",
    "    \"\"\"Builds a simple shared convolutional model.\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # Add some convolutional layers\n",
    "    return Model(inputs, x)\n",
    "\n",
    "def build_siamese_model(shared_conv, input_shape=(60, 60, 3)):\n",
    "    \"\"\"Builds a siamese model with the given shared convolutional base.\"\"\"\n",
    "\n",
    "    # We need two separate input layers, one for each picture\n",
    "    \n",
    "    # Then we run each picture through the shared model to get two outputs\n",
    "\n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([output_1, output_2])\n",
    "\n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    output = Dense(1, activation='sigmoid')(L1_distance)\n",
    "\n",
    "    model = Model(inputs=[input_1, input_2], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "shared_conv = build_shared_conv()\n",
    "model = build_siamese_model(shared_conv)\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95u6Xt6UnZx5",
    "outputId": "1f0fde0a-92c5-4a18-d130-c226655ac893"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[accuracy_sim])"
   ],
   "metadata": {
    "id": "20EAuMy8nZx5"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaUKfVSLnZx5"
   },
   "source": [
    "We can now fit the model and checkpoint it to keep the best version. We can expect to get a model with around 0.75 as \"accuracy_sim\" on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gf5j84KnZx6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "best_model_cb = ModelCheckpoint(\n",
    "    \"siamese_checkpoint\",\n",
    "    monitor='val_accuracy_sim',\n",
    "    save_best_only=True, verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_cb = EarlyStopping(\n",
    "    monitor='val_accuracy_sim',\n",
    "    patience=5, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxKRqGU-nZx6",
    "outputId": "c8baf860-2509-4236-9704-9d60a6d28cd1"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "          gen,\n",
    "          epochs=50,\n",
    "          validation_data=([test_X1, test_X2], Y_test),\n",
    "          callbacks=[best_model_cb, early_stopping_cb]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQL8A22gnZx6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"siamese_checkpoint\", custom_objects={\"accuracy_sim\": accuracy_sim})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIR80FAJnZx7"
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "Finding the most similar images\n",
    "\n",
    "- Run the shared_conv model on all images;\n",
    "- build a `most_sim` function which returns the most similar vectors to a given vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EF-foSsfnZx7"
   },
   "outputs": [],
   "source": [
    "all_images_path = []\n",
    "for img_list in img_paths.values():\n",
    "    all_images_path += img_list\n",
    "path_to_id = {v: k for k, v in enumerate(all_images_path)}\n",
    "id_to_path = {v: k for k, v in path_to_id.items()}\n",
    "all_imgs = open_all_images(id_to_path)\n",
    "\n",
    "# Actually compute the similarities\n",
    "emb = shared_conv(all_imgs)\n",
    "emb = emb / np.linalg.norm(emb, axis=-1, keepdims=True)\n",
    "\n",
    "def most_sim(x, emb, topn=4):\n",
    "    sims = np.dot(emb, x)\n",
    "    ids = np.argsort(sims)[::-1]\n",
    "    return [(id, sims[id]) for id in ids[:topn]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eokXTn8OnZx7"
   },
   "source": [
    "**Most similar faces **\n",
    "\n",
    "The following enables to display an image alongside with most similar images:\n",
    "\n",
    "- The results are weak, first because of the size of the dataset\n",
    "- Also, the network can be greatly improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-acxB9NVnZx7"
   },
   "outputs": [],
   "source": [
    "def display(img):\n",
    "    img = img.astype('uint8')\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fldzM7f_nZx8",
    "outputId": "aed1e53c-f912-47ef-dca8-1d658cd05fa1"
   },
   "outputs": [],
   "source": [
    "interesting_classes = list(filter(lambda x: len(x[1]) > 4, classid_to_ids.items()))\n",
    "class_id = random.choice(interesting_classes)[0]\n",
    "\n",
    "query_id = random.choice(classid_to_ids[class_id])\n",
    "print(\"query:\", classid_to_name[class_id], query_id)\n",
    "# display(all_imgs[query_id])\n",
    "\n",
    "print(\"nearest matches\")\n",
    "for result_id, sim in most_sim(emb[query_id], emb):\n",
    "    class_name = classid_to_name.get(id_to_classid.get(result_id))\n",
    "    print(class_name, result_id, sim)\n",
    "    display(all_imgs[result_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YCZSbX1nZx8"
   },
   "source": [
    "Note that this model is still underfitting, even when running queries against the training set. Even if the results are not correct, the mistakes often seem to \"make sense\" though.\n",
    "\n",
    "Running a model to convergence on higher resolution images, possibly with a deeper and wider convolutional network might yield better results. In the next notebook we will try with a better loss and with hard negative mining."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
