{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Day 1: Introduction to Machine Learning with Python\n",
    "## Lab 1-1: Cleaning and Processing Data\n",
    "\n",
    "Welcome to the very first lab of the Machine Learning workshop! In this lab, we will cover the basics of data cleaning and processing. We'll start by looking at a popular toy dataset for machine learning - the Titanic dataset. Then, we will move on to looking at loading and handling FASTA and FASTQ files, which are commonly used in bioinformatics.\n",
    "\n",
    "### The Titanic Dataset\n",
    "\n",
    "The Titanic dataset is a popular practice dataset for machine learning. The dataset contains information about passengers on the Titanic, such as their age, ticket class, name, and crucially whether they survived or not. Generally, this dataset is used to build a model to predict whether a passenger survived or not based on the other information available.\n",
    "\n",
    "However, many of the columns in the dataset contain missing values, which can cause problems when building a machine learning model. In this lab, we will learn about a few of the different ways to handle missing values in a dataset.\n",
    "\n",
    "### Scikit-Learn\n",
    "\n",
    "Scikit-Learn is a popular machine learning library in Python. It provides a wide range of tools for building machine learning models, including tools for data preprocessing, model building, and model evaluation. We will take advantage of Scikit-Learn's tools in this lab to handle missing values in the Titanic dataset. Let's start by loading the data, and inspecting it using Pandas:"
   ],
   "id": "52e575b17eac4f55"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic = fetch_openml(name='titanic', version=1)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "df = pd.DataFrame(titanic.data, columns=titanic.feature_names)\n",
    "# Add survival information to the DataFrame\n",
    "df['survived'] = titanic.target"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Look at some of the data\n",
    "df"
   ],
   "id": "cc6a3f76a00f3ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Take a moment to familiarize yourself with the contents of the dataset. Try to answer some of the following questions. If you are new to Python, work in a group with someone who has familiarity with Python, or ask for help!\n",
    "\n",
    "1. What percentage of passengers survived?\n",
    "2. What was the average age of passengers?\n",
    "3. What was the most common ticket class?"
   ],
   "id": "945b71cc97e9519f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here",
   "id": "32e81c19fccae62f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note: At this stage we are also going to remove the `name`, `ticket`, `cabin`, `boat` and `home.dest` columns, as these contain non-numeric data that is difficult to work with. We will come back to these columns in a later lab.",
   "id": "17fa6a99d82bbf38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove non-numeric columns\n",
    "df = df.drop(columns=['name', 'ticket', 'cabin', 'boat', 'home.dest'])"
   ],
   "id": "36bba7039d94f6ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There is one other thing we need to do before we can move forward. The model we will build requires that all of the data be numeric, but we have a few columns with text in them. The `sex` and `embarked` columns are examples of this. We can convert these columns to numeric values using a technique called \"one-hot encoding\". This technique converts each unique value in a column to a new column, and assigns a 1 or 0 to each new column depending on whether the original column contained that value. So for example, we can replace the `sex` column with two new columns, `sex_male` and `sex_female`, one of which will be 1 and the other 0 for each row (note: we can actually get away with one fewer column than the number of unique values in the original column, but we'll ignore that for now. Think about why this is the case).",
   "id": "8c6dbb701e57e4ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Perform one-hot encoding on the 'sex' and 'embarked' columns\n",
    "df = pd.get_dummies(df, columns=['sex', 'embarked'])\n",
    "\n",
    "# Look at the data again\n",
    "df"
   ],
   "id": "f0e484a0d1b00137",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "One of the most common problems in real-world datasets is missing values. Missing values can cause problems when building machine learning models, so it is important to handle them properly. There are several ways to handle missing values, including:\n",
    "\n",
    "1. Removing rows or columns with missing values\n",
    "2. \"Imputing\" missing values by filling them in with a best guess\n",
    "3. Using a model that treats missing values as a separate category\n",
    "\n",
    "We are going to look at each of these methods in turn, and see how they affect the performance of a machine learning model.\n",
    "\n",
    "### Removing Rows with Missing Values\n",
    "\n",
    "The simplest way to handle missing values is to remove any rows that contain missing values. This is a quick and easy way to handle missing values, but it can also lead to a loss of information. Pandas makes it easy to remove rows with missing values using the `dropna()` method. Let's see how this affects the Titanic dataset:"
   ],
   "id": "5d6f6b1f905ce07a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "\n",
    "# Look at the shape of the original and modified data\n",
    "print(f'Original data shape: {df.shape}')\n",
    "print(f'Modified data shape: {df_dropped.shape}')"
   ],
   "id": "31c600b5bdd4ca5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "...whoops! It looks like we have lost the significant majority of our dataset with this approach. If we inspect the data further, we can see which columns contain a significant number of missing values:",
   "id": "30e84300d8de8789"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Report the number of missing values by column\n",
    "print(df.isnull().sum()/len(df) * 100)"
   ],
   "id": "cb320a5215ea5fd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Based on this we can see that the `body` column contains a large number of missing values. This column encodes whether the body was recovered. It is not surprising that this information is missing for many passengers. Let's see what happens if we remove this column entirely, and then remove rows with missing values:",
   "id": "d4368a6dd3f41037"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove columns with a large number of missing values (and non-numeric columns)\n",
    "df_removed = df.drop(columns=['body'])\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_removed = df_removed.dropna()\n",
    "\n",
    "# Look at the shape of the original and modified data\n",
    "print(f'Original data shape: {df.shape}')\n",
    "print(f'Modified data shape: {df_removed.shape}')"
   ],
   "id": "b2ea5327008f4fde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Okay, that loss of data is a bit easier to work with. We now know that every row in the dataset has complete information. There's an obvious tradeoff here, in that we have to decide whether it is better to have more data with missing values, or less data with complete information. However, there's also a slightly more subtle risk that this approach introduces. Can you think of what it might be? We'll come back to it later.\n",
    "\n",
    "For now, let's build a very simple machine learning model to predict whether a passenger survived or not. We'll use the `RandomForestClassifier` model from Scikit-Learn, which is a popular model for classification tasks. We'll start by splitting the data into features and labels, and then splitting the data into training and testing sets:"
   ],
   "id": "45db52ed14500b35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = df_removed.drop(columns='survived')\n",
    "y = df_removed['survived']"
   ],
   "id": "24554553fea40c92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ],
   "id": "870f792835b40f1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f'Model accuracy: {accuracy:.4f}')"
   ],
   "id": "8817efef5f5a947f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There is a bit of randomness here based on how we split the data, but hopefully you should see an accuracy of around 80%. This is not bad! It's worth also breaking this down based on whether the passenger survived or not, as this can give us a better idea of how well the model is performing:",
   "id": "505186a448304567"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'Model accuracy for passengers who did not survive: {model.score(X_test[y_test == \"0\"], y_test[y_test == \"0\"]):.4f}')\n",
    "print(f'Model accuracy for passengers who survived:        {model.score(X_test[y_test == \"1\"], y_test[y_test == \"1\"]):.4f}')"
   ],
   "id": "ad3535a5c41dd327",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we go through the workshop, we will discuss further the idea that different ways of measuring performance can be pretty impactful. For now, it's just worth considering how this difference in performance might be important depending on what the model is being used for. What could be a situation where this difference in performance might be particularly important?\n",
    "\n",
    "### Imputing Missing Values\n",
    "\n",
    "Another way to handle missing values is to \"impute\" them, which means filling them in with a best guess. There are many ways to impute missing values, but one common way is to fill them in with the mean or median of the column. This is slightly more sophisticated than just removing rows with missing values, as it allows us to keep more data. Essentially, what we are doing is building a very simple model to predict the missing values based on the data we do have.\n",
    "\n",
    "The simplest type of imputation is to fill in missing values with the mean of the column. This is easy to do with Pandas:"
   ],
   "id": "759c532d03c1bdb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make a copy of our dataframe for imputation\n",
    "df_imputed = df.copy()"
   ],
   "id": "b584cfc6822a6bd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for column in df.columns:\n",
    "    if df_imputed[column].isnull().sum() > 0:\n",
    "        print(f'Imputing missing values for \"{column}\", which has {df_imputed[column].isnull().sum()} missing value(s)')\n",
    "        mean = df_imputed[df_imputed[column].notnull()][column].mean()\n",
    "        print(f'  Mean value: {mean:.2f}')\n",
    "        df_imputed[column] = df_imputed[column].fillna(mean)"
   ],
   "id": "622ba95b5a0f0dff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "An obvious flaw here is that we are just using the same mean for all missing values in a column. This is a very simple approach, and there are many more sophisticated ways to impute missing values. However, this is a good starting point. Let's see how this affects the performance of our model:",
   "id": "627825ede9026ddb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the data into features and labels\n",
    "X = df_imputed.drop(columns='survived')\n",
    "y = df_imputed['survived']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f'Model accuracy: {accuracy:.4f}')"
   ],
   "id": "3c1210c9e6a46f59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'Model accuracy for passengers who did not survive: {model.score(X_test[y_test == \"0\"], y_test[y_test == \"0\"]):.4f}')\n",
    "print(f'Model accuracy for passengers who survived:        {model.score(X_test[y_test == \"1\"], y_test[y_test == \"1\"]):.4f}')"
   ],
   "id": "e50ce08322761c2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Compare the performance of the model with imputed missing values to the performance of the model with missing values removed. What do you notice? What are the tradeoffs between these two approaches?\n",
    "\n",
    "### Using a Model to Handle Missing Values\n",
    "\n",
    "Another way to handle missing values is to use a model that treats missing values as a separate category. As we alluded to earlier, there can be a significant cost to removing rows with missing values: not only are we losing information, but often there can be an underlying reason why the data is missing. For example, in the Titanic dataset, the `body` column is missing for many passengers because their body was never recovered. This is not a random process, and removing these rows could introduce bias into our model.\n",
    "\n",
    "In Scikit-Learn, unlike `RandomForestClassifier`, there are models that can handle missing values directly. One such model is `HistGradientBoostingClassifier`. This model can handle missing values by treating them as a separate category. Let's see how this model performs on the Titanic dataset:"
   ],
   "id": "48583d50bb9d8a24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the data into features and labels\n",
    "X = df.drop(columns='survived')\n",
    "y = df['survived']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ],
   "id": "6d93fea725da90ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Train a Gradient Boosting model\n",
    "model = HistGradientBoostingClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f'Model accuracy: {accuracy:.4f}')"
   ],
   "id": "f8c85126e3d8bed3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'Model accuracy for passengers who did not survive: {model.score(X_test[y_test == \"0\"], y_test[y_test == \"0\"]):.4f}')\n",
    "print(f'Model accuracy for passengers who survived:        {model.score(X_test[y_test == \"1\"], y_test[y_test == \"1\"]):.4f}')"
   ],
   "id": "341b3291041c0aff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "How does the performance of the `HistGradientBoostingClassifier` model compare to the `RandomForestClassifier` model? What are the tradeoffs between these two approaches?\n",
    "\n",
    "### Deciding on the Best Approach\n",
    "\n",
    "There is no one-size-fits-all approach to handling missing values in a dataset. The best approach depends on the dataset, the problem you are trying to solve, and the model you are using. In general, it is a good idea to try multiple approaches and see which one works best for your particular problem. Some of the things that you might want to consider when deciding on an approach include:\n",
    "\n",
    "- The amount of missing data in the dataset: if there are only a handful of missing values, it might be best to just remove them. If there are a large number of missing values, it might be better to impute them.\n",
    "- The underlying reason for the missing data: could there be a systemic reason why the data is missing? If so, removing the missing values could introduce bias into the model.\n",
    "- The model you are using: some models can handle missing values directly, while others cannot. It is a good idea to choose a model that can handle missing values if you have a large amount of missing data.\n",
    "\n",
    "## FASTA and FASTQ Files\n",
    "\n",
    "FASTA and FASTQ files are commonly used in bioinformatics to store DNA and protein sequences. FASTA files store sequences in a simple text format, while FASTQ files store sequences along with quality scores for each base in the sequence. In this section, we will look at how to load and handle FASTA and FASTQ files in Python.\n",
    "\n",
    "### Loading FASTA Files\n",
    "\n",
    "FASTA files store sequences in a simple text format. Each sequence is represented by a header line starting with a `>` character, followed by one or more lines containing the sequence itself. We can use the `Biopython` library to load and handle FASTA files in Python. Let's start by loading a FASTA file and looking at the sequences it contains. First we need to install the library:"
   ],
   "id": "2401f413c3fa4ad5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -U biopython",
   "id": "c0d81aff176e783d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "# Load a FASTA file\n",
    "fasta_file = 'NM_001323632.2.fasta'\n",
    "records = list(SeqIO.parse(fasta_file, 'fasta'))"
   ],
   "id": "ff19e2ce247fa6c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Look at the first record\n",
    "record = records[0]\n",
    "print(f'ID: {record.id}')\n",
    "print(f'Description: {record.description}')\n",
    "print(f'Sequence: {record.seq}')"
   ],
   "id": "2c4b1db9bee185cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once we have loaded the FASTA file, we can access the sequences using the `SeqRecord` object. The `SeqRecord` object has several attributes, including `id`, `description`, and `seq`, which contain the ID of the sequence, a description of the sequence, and the sequence itself, respectively. We can do some simple processing on the sequences, such as calculating the length of the sequence, or counting the number of each base in the sequence:",
   "id": "6bffcb06700b7cb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the length of the sequence\n",
    "length = len(record.seq)\n",
    "\n",
    "# Count the number of each base in the sequence\n",
    "counts = {base: record.seq.count(base) for base in 'ACGT'}"
   ],
   "id": "b71f6d093876c25b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'Sequence length: {length}')\n",
    "print(f'Base counts: {counts}')\n",
    "# Calculate the GC content of the sequence\n",
    "gc_content = (counts['G'] + counts['C']) / length\n",
    "print(f'GC content: {gc_content:.2f}')"
   ],
   "id": "f423b1eec8dac468",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loading FASTQ Files\n",
    "\n",
    "FASTQ files are similar to FASTA files, but they also contain quality scores for each base in the sequence. Quality scores are used to estimate the probability that a base is called incorrectly. We can use the `Biopython` library to load and handle FASTQ files in Python. Let's start by loading a FASTQ file and looking at the sequences and quality scores it contains:"
   ],
   "id": "69cd8fda5c56d848"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fastq_file = 'SRR000129.fastq'\n",
    "records = list(SeqIO.parse(fastq_file, 'fastq'))"
   ],
   "id": "d678ed3b1e1ed1d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "record = records[0]\n",
    "print(f'ID: {record.id}')\n",
    "print(f'Description: {record.description}')\n",
    "print(f'Sequence: {record.seq}')\n",
    "print(f'Quality scores: {record.letter_annotations[\"phred_quality\"]}')"
   ],
   "id": "c82dfaca4c885e75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the average quality score for the sequence\n",
    "average_quality = sum(record.letter_annotations['phred_quality']) / len(record.letter_annotations['phred_quality'])\n",
    "print(f'Average quality score: {average_quality:.2f}')"
   ],
   "id": "53fe61ed893f50db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the GC content of the sequence\n",
    "length = len(record.seq)\n",
    "counts = {base: record.seq.count(base) for base in 'ACGT'}\n",
    "gc_content = (counts['G'] + counts['C']) / length\n",
    "print(f'GC content: {gc_content:.2f}')"
   ],
   "id": "6c1c183d282f5ae3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the average quality score for each base in the sequence\n",
    "average_quality = {base: sum(record.letter_annotations['phred_quality'][i] for i, b in enumerate(record.seq) if b == base) / counts[base] for base in 'ACGT'}\n",
    "print(f'Average quality scores: {average_quality}')"
   ],
   "id": "6c9ac310da386a03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "In this lab, we have looked at some of the different conceptual approaches in machine learning to handling missing data. We have also looked at how to load and handle FASTA and FASTQ files in Python. In the next lab, we will look in more detail at building machine learning models using Scikit-Learn."
   ],
   "id": "ddde06ddb2165d6e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
