{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Building a Neural Network with PyTorch and PyTorch Lightning\n",
    "\n",
    "In this lab, we will dive into building our own neural network using PyTorch and PyTorch Lightning. We'll work with our HCC data and build a simple neural network to predict whether a sample has cancer or not."
   ],
   "id": "fb589c74dd9bc021"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T19:15:31.335305Z",
     "start_time": "2024-04-13T19:15:27.976099Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -U -q torch torchvision pytorch-lightning",
   "id": "21ec437bde7496d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-04-13T19:15:31.338907Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "hcc = pd.read_csv('https://github.com/alexwolson/carte_workshop_2024/raw/main/data/HCC_all_ML_classification_test_annotated_frags_all_features_combined_4_tumors.csv.gz', compression='gzip')\n",
    "hcc = hcc.sample(500)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categorical_columns = ['chr','upstream_motif','downstream_motif','Corrected_Call']\n",
    "numerical_columns = ['frag','VAF','pos','read_cov','detected_read_cov','plasma_VAF','Corrected_Copy_Number']\n",
    "y_column = 'alt_match'\n",
    "\n",
    "X = hcc[categorical_columns + numerical_columns]\n",
    "y = hcc[y_column]"
   ],
   "id": "c2bd43eb84e56145",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X = pd.get_dummies(X, columns=categorical_columns)",
   "id": "3d330231ca32c155",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ],
   "id": "762ae568e856706",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building a Neural Network with PyTorch\n",
    "\n",
    "PyTorch is a popular deep learning framework that allows you to build neural networks. PyTorch is a bit more low-level than TensorFlow, which handles some of the details for you. However, PyTorch is more flexible and allows you to build custom neural networks with ease.\n",
    "\n",
    "In order to keep things simple, we will also take advantage of a companion library called PyTorch Lightning. Lightning takes care of much of the code that normally needs to be written by hand, allowing you to focus on building your neural network. Despite this, we will still need to write a fair amount of code to build our neural network.\n",
    "\n",
    "Let's start by defining a simple, fully connected neural network using PyTorch."
   ],
   "id": "4a0190c63d2f7e51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as L\n",
    "\n",
    "class NN(L.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 64)\n",
    "        # Add additional layers following the same format. Bring us down to a single output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass of the network\n",
    "        # Alternate between linear layers and activation functions\n",
    "        # Relu can be called using F.relu\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy(y_hat, y.view(-1, 1))\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ],
   "id": "11bad6b59c0b07ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training the Model\n",
    "\n",
    "Now that we have defined our neural network, we can train it using PyTorch Lightning. We will create a PyTorch Dataset and DataLoader to feed our data into the model, and then use a Trainer to train the model. The Dataset and DataLoader classes have us define exactly how our data should be retrieved and fed into the model, while the Trainer class handles the training loop for us."
   ],
   "id": "a43d89363706f236"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class HCCDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # This is the important function that defines how data is retrieved from the dataset\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "id": "a3e5c7b7c81a0b4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset = HCCDataset(X_train.astype(float), y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = HCCDataset(X_val.astype(float), y_val)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = NN()\n",
    "trainer = L.Trainer(max_epochs=10, log_every_n_steps=1)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ],
   "id": "ee35a1f8dbc4b51f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Now that we have trained our model, we can evaluate it on the test set to see how well it performs."
   ],
   "id": "b3289fd4a54fade7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_dataset = HCCDataset(X_test.astype(float), y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "id": "8325825788f62aeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Just like how we defined a training loop for our model, we have to define an evaluation loop to evaluate the model on the test set. There are a few key elements to this:\n",
    "\n",
    "1. We need to set the model to evaluation mode using `model.eval()`. This tells PyTorch that we are evaluating the model and not training it.\n",
    "2. We need to use the `torch.no_grad()` context manager to tell PyTorch that we do not need to keep track of gradients during evaluation. This can save memory and speed up computation.\n",
    "3. We need to loop over the test set and compute predictions using the model. We can then use these predictions to compute the accuracy of the model."
   ],
   "id": "391f29418be64fd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Switch the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Use torch.no_grad() to disable gradient tracking\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # Loop over the test set\n",
    "    for x, y in test_loader:\n",
    "        y_hat = model(x) # Compute predictions\n",
    "        y_hat = (y_hat > 0.5).float() # Convert to binary predictions\n",
    "        \n",
    "        # Append the true and predicted labels to the lists\n",
    "        y_true.extend(y.numpy())\n",
    "        y_pred.extend(y_hat.numpy().flatten())"
   ],
   "id": "6be4e443a6e11f6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now `y_pred` and `y_true` contain the predicted and true labels for the test set. We can use these to compute the accuracy of the model.",
   "id": "feb05bf218f94d38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute the accuracy of the model\n",
    "\n",
    "# ===== YOUR CODE HERE =====\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =========================="
   ],
   "id": "b94e8ca4a4f35be0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extending our model further\n",
    "\n",
    "Now that we have a simple neural network that can predict whether a sample has cancer or not, we can extend it further to improve its performance. Here are a few ideas for how you can extend the model:\n",
    "\n",
    "1. Add more layers to the neural network. You can experiment with different widths, depths and activations to see how they affect the performance of the model.\n",
    "2. Experiment with different optimization algorithms. You can try using different optimizers such as SGD, RMSprop, or Adam, and experiment with different learning rates.\n",
    "3. There are many settings that can be configured with the Lightning Trainer. Take a look at the [callbacks](https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html) (particularly EarlyStopping and ModelCheckpoint) and [logging](https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html) options to see how you can monitor and improve your model during training."
   ],
   "id": "d9cfa263454da4d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1dabfcadac787553",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tracking Experiments\n",
    "\n",
    "Often, we want to be able to keep track of the experiments we run, including the hyperparameters, metrics, and other information. One way to do this is to use a tool like [Weights & Biases](https://wandb.ai/site) (wandb) to log and visualize our experiments.\n",
    "\n",
    "In order to use wandb, you will need to sign up for an account and install the wandb library. You can do this by running `!pip install wandb` in a code cell."
   ],
   "id": "219a855d9acfb45b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install wandb",
   "id": "33a71583b4cf9cb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "wandb.login()"
   ],
   "id": "aeec214c0c11e40d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize a new wandb run\n",
    "\n",
    "run = wandb.init(project='hcc', config={\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 10\n",
    "})"
   ],
   "id": "d028ab3d54e2ad89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we've initialized a wandb run, we can log information about our model and training process. Pytorch Lightning has built-in support for wandb, so we can easily log metrics, hyperparameters, and other information during training.",
   "id": "f4b0304738e6cb1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(log_model=True) # Save the model as an artifact\n",
    "\n",
    "model = NN()\n",
    "\n",
    "trainer = L.Trainer(max_epochs=10, logger=wandb_logger)"
   ],
   "id": "443cb02702e4c74d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once we have trained our model, we can log the final metrics and any other information we want to keep track of. You'll then be able to view this information in the wandb dashboard: [https://wandb.ai/home](https://wandb.ai/home)",
   "id": "cb57a337dde6435a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "dc9eb8bc07a87b43",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
