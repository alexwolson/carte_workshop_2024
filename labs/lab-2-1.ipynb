{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Day 2: Data Visualization and Scikit-Learn\n",
    "# Lab 2-1: Visualizing high-dimensional data\n",
    "\n",
    "In this lab, we are going to look at a few different methods for visualizing high dimensional data. We will use the MNIST dataset, which is a dataset of 28x28 pixel images of handwritten digits. Each image is a 28x28 pixel array, which we will flatten into a 784-dimensional vector. Let's take a look at the data:"
   ],
   "id": "c4914e633afe5b10"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = fetch_openml('mnist_784', parser='auto')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Working with HCC data\n",
    "\n",
    "As an alternative, we can load in some real-world HCC data and apply these techniques to that data. Run the code below to load the dataset in - you'll need to apply some pre-processing techniques before you can run PCA, t-SNE, or UMAP on it: drop columns that are unsuitable, encode categorical columns, and check for missing values."
   ],
   "id": "38a99e203c292eea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "hcc = pd.read_csv('https://github.com/alexwolson/carte_workshop_2024/raw/main/data/HCC_all_ML_classification_test_annotated_frags_all_features_combined_4_tumors.csv.gz', compression='gzip')\n",
    "hcc.head(100)"
   ],
   "id": "96942b6ef5966f55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's take a look at a few different samples\n",
    "fig, axes = plt.subplots(3,3, figsize=(10,10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(mnist.data.values[i].reshape(28,28), cmap='gray_r')\n",
    "    ax.set_title(mnist.target.values[i])\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ],
   "id": "97915bbe00c46c1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PCA\n",
    "\n",
    "Before we apply PCA to our data, let's make sure we understand it. We will start with some randomly generated points:"
   ],
   "id": "ec3760d1c39caba0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(1) # set the random state for reproducability\n",
    "\n",
    "X = np.dot(rng.rand(2,2),\n",
    "           rng.randn(2,200)).T # create synthetic data\n",
    "plt.scatter(X[:,0],X[:,1]) # plot synthetic data\n",
    "plt.axis('equal');"
   ],
   "id": "8169087afd48c646",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here we have a two dimensional dataset that, by eye, has a nearly linear relationship between the X and Y values. This is reminiscent of the type of data we would look to explore with linear regression, but the problem setting here is slightly different: rather than attempting to predict the y values from the x values, the unsupervised learning problem attempts to learn about the relationship between the x and y values.\n",
    "\n",
    "In principal component analysis, this relationship is quantified by finding a list of the principal axes in the data, and using those axes to describe the dataset. Using Scikit-Learn's PCA estimator, we can compute this as follows:"
   ],
   "id": "e610fac3b5deaeff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X);"
   ],
   "id": "d69b643bc52aad35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The fit learns some quantities from the data, most importantly the \"components\" and \"explained variance\":   ",
   "id": "8bd579028cf6d7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'Components:\\n{pca.components_}\\n')\n",
    "print(f'Explained variance: {pca.explained_variance_}')"
   ],
   "id": "ad829da37e1e2782",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To see what these numbers mean, let's visualize them as vectors over the input data, using the \"components\" to define the direction of the vector, and the \"explained variance\" to define the squared-length of the vector:",
   "id": "417918059d31ba0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color='black')\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ],
   "id": "c21f80158f7d70a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These vectors represent the principal axes of the data, and the length of the vector is an indication of how \"important\" that axis is in describing the distribution of the data—more precisely, it is a measure of the variance of the data when projected onto that axis. The projection of each data point onto the principal axes are the \"principal components\" of the data.\n",
    "\n",
    "You have likely noticed that in this example, almost all of the variance is explained by the first axis, while the second axis explains very little variance. This is where PCA becomes a tool for dimensionality reduction. By transforming the data to lie along the PCA dimensions and then only keeping the first K dimensions, we can reduce our dataset into K dimensional space:"
   ],
   "id": "8561702a53112db4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)\n",
    "plt.hist(X_pca,bins=20)\n",
    "plt.xlabel('Value in reduced space')\n",
    "plt.ylabel('Count');"
   ],
   "id": "10a274ebe34ab07b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If we project our new, low-dimensional data back into the original space, we can get a good visual intuition of what PCA has done for us:",
   "id": "79e0a155eb088507"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
    "plt.axis('equal');"
   ],
   "id": "d9453ded10002766",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The light points are the original data, while the orange points are the projected version. This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n",
    "\n",
    "This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved.\n",
    "\n",
    "The usefulness of the dimensionality reduction may not be entirely apparent in only two dimensions, but becomes much more clear when looking at high-dimensional data. To see this, let's now look at applying PCA to our digits.\n",
    "\n",
    "**Your Turn**\n",
    "\n",
    "The code above shows how to create a PCA model and apply it to a dataset. Now, apply PCA to the MNIST dataset, reducing the dimensionality to 2 dimensions. Then, use the method supplied below to plot your results. What do you see?"
   ],
   "id": "9e28212d7f5bf750"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_digits(data, target):\n",
    "    if data.shape[1] > 2:\n",
    "        raise ValueError(\"Data has more than 2 dimensions - PCA must be applied first\")\n",
    "    plt.figure(figsize=(12,12))\n",
    "    sns.scatterplot(x=data[:,0], y=data[:,1], hue=target, palette='nipy_spectral')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.title('2D PCA of MNIST Dataset')\n",
    "    plt.show()\n",
    "\n",
    "# ======= Your code here ========\n",
    "\n",
    "# Make sure to apply PCA to the MNIST dataset and then plot the results using the function above\n",
    "\n",
    "# ================================"
   ],
   "id": "3c4e2ba3fe52e570",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## t-SNE\n",
    "\n",
    "We are now going to look at a different method of dimensionality reduction: t-SNE, a method partially developed here at U of T by Geoffrey Hinton in 2008. It differs from PCA in that it uses the _local relationships_ between points to create a low-dimensional mapping. Among other things, this allows t-SNE to capture non-linear structures in the data.\n",
    "\n",
    "There are a number of other benefits to t-SNE over PCA, but since we are focusing on using dimensionality reduction for visualization we will stick to the benefits which are apparent in that space. Because t-SNE considers the local relationships between points, it ensures that the distances between points in the low dimensional mapping are representative of the distances in the original space. This makes it a lot more useful for visualization compared to PCA!\n",
    "\n",
    "t-SNE – at a high level – basically works like this:\n",
    "\n",
    "Step 1: In the high-dimensional space, create a probability distribution that dictates the relationships between various neighboring points\n",
    "\n",
    "Step 2: It then tries to recreate a low dimensional space that follows that probability distribution as best as possible.\n",
    "\n",
    "The “t” in t-SNE comes from the t-distribution, which is the distribution used in Step 2. The “S” and “N” (“stochastic” and “neighbor”) come from the fact that it uses a probability distribution across neighboring points.\n",
    "\n",
    "Let's apply t-SNE to the MNIST dataset and see how it compares to PCA. One thing to note about t-SNE is that it's much, much slower than PCA. We will time our execution to get a sense of what that looks like in practice."
   ],
   "id": "408f9d467f6a3580"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from time import time",
   "id": "8a1257d34304388",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute PCA from before, for comparison\n",
    "t0 = time()\n",
    "X_pca = PCA(n_components=2).fit_transform(mnist.data.values)\n",
    "print(f'PCA done in {time()-t0:.4f} seconds')"
   ],
   "id": "ea47d0f50a61c78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Because I don't want you to have to run this over and over again if you restart the notebook, we are going to write some code that will save your results on completion (and try to load that back up if it exists). This way, you can just run the t-SNE code once and then play around with the results as much as you want.",
   "id": "41cb7bfa93e12529"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tsne_path = Path('tsne.npy') # path to save t-SNE results"
   ],
   "id": "ebe139a9b6d17c05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute t-SNE\n",
    "if tsne_path.exists(): # if the file exists, load it\n",
    "    X_tsne = np.load(tsne_path)\n",
    "else: # otherwise, compute it\n",
    "    from sklearn.manifold import TSNE\n",
    "    t0 = time()\n",
    "    X_tsne = TSNE(\n",
    "        n_components=2, \n",
    "        verbose=1,\n",
    "        n_jobs=-1 # use all CPU cores\n",
    "    ).fit_transform(mnist.data.values)\n",
    "    print(f't-SNE done in {time()-t0:.4f} seconds')\n",
    "    np.save(tsne_path, X_tsne)"
   ],
   "id": "69b58485dcd13c9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that t-SNE is done, let's plot the results and see how it compares to PCA:",
   "id": "87647e591c8f0788"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(20,10))\n",
    "\n",
    "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=mnist.target.values, palette='nipy_spectral', ax=axes[0])\n",
    "axes[0].set_title('2D PCA of MNIST Dataset')\n",
    "axes[0].set_xlabel('First Principal Component')\n",
    "axes[0].set_ylabel('Second Principal Component')\n",
    "\n",
    "sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=mnist.target.values, palette='nipy_spectral', ax=axes[1])\n",
    "axes[1].set_title('2D t-SNE of MNIST Dataset')\n",
    "axes[1].set_xlabel('First t-SNE Component')\n",
    "axes[1].set_ylabel('Second t-SNE Component')\n",
    "\n",
    "plt.show()"
   ],
   "id": "3d8b9bbbd665d2dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You should be able to see that t-SNE has done a much better job of separating the different classes of digits compared to PCA. This is because t-SNE is able to capture the non-linear relationships between the points, while PCA is only able to capture the linear relationships.\n",
    "\n",
    "## UMAP\n",
    "\n",
    "UMAP is another dimensionality reduction technique that is similar to t-SNE, but is generally faster and has some other benefits. It is also able to capture non-linear relationships between points, and is generally considered to be a good alternative to t-SNE.\n",
    "\n",
    "UMAP differs from t-SNE in that it tries to optimize structure based on a graph representation of the data, rather than a probability distribution. This allows UMAP to be more scalable and generally faster than t-SNE.\n",
    "\n",
    "Let's apply UMAP to the MNIST dataset and see how it compares to PCA and t-SNE. You'll need to install the UMAP library first:"
   ],
   "id": "1948275645dff11f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -U umap-learn",
   "id": "124c9819c6798ba6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from umap import UMAP\n",
    "\n",
    "t0 = time()\n",
    "# ========= YOUR TURN ==========\n",
    "# Compute UMAP on the MNIST dataset\n",
    "# ==============================\n",
    "print(f'UMAP done in {time()-t0:.4f} seconds')"
   ],
   "id": "cf54f222e9ccb86c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(30,10))\n",
    "\n",
    "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=mnist.target.values, palette='nipy_spectral', ax=axes[0])\n",
    "axes[0].set_title('2D PCA of MNIST Dataset')\n",
    "axes[0].set_xlabel('First Principal Component')\n",
    "axes[0].set_ylabel('Second Principal Component')\n",
    "\n",
    "sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=mnist.target.values, palette='nipy_spectral', ax=axes[1])\n",
    "axes[1].set_title('2D t-SNE of MNIST Dataset')\n",
    "axes[1].set_xlabel('First t-SNE Component')\n",
    "axes[1].set_ylabel('Second t-SNE Component')\n",
    "\n",
    "sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], hue=mnist.target.values, palette='nipy_spectral', ax=axes[2])\n",
    "axes[2].set_title('2D UMAP of MNIST Dataset')\n",
    "axes[2].set_xlabel('First UMAP Component')\n",
    "axes[2].set_ylabel('Second UMAP Component')\n",
    "\n",
    "plt.show()"
   ],
   "id": "4312808bf94a01e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You should see that UMAP has done a good job of separating the different classes of digits, similar to t-SNE. UMAP is generally faster than t-SNE, so it is a good alternative when you have a large dataset.\n",
    "\n",
    "## Bonus: Visualizing high-dimensional data using autoencoders\n",
    "\n",
    "Looking ahead to later in the course, we will learn about autoencoders, which are a type of neural network that can be used for dimensionality reduction. Autoencoders are able to capture non-linear relationships between points, similar to t-SNE and UMAP.\n",
    "\n",
    "An autoencoder is comprised of two parts: an encoder and a decoder. The encoder takes the high-dimensional data and maps it to a lower-dimensional space, while the decoder takes the lower-dimensional data and maps it back to the high-dimensional space. The goal of the autoencoder is to minimize the difference between the input data and the output data. \n",
    "\n",
    "You can think of this as trying to compress the data into a lower-dimensional space, and then reconstructing the data from that lower-dimensional space. This is similar to PCA, but autoencoders are able to capture non-linear relationships between points, like t-SNE and UMAP.\n",
    "\n",
    "Let's build a very simple autoencoder using Keras and apply it to the MNIST dataset. We will use a 2-layer neural network for the encoder and a 2-layer neural network for the decoder. We will then use the encoder to map the data to a 2-dimensional space, and plot the results."
   ],
   "id": "a0ca04b0e90d2313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Build the autoencoder\n",
    "encoder = models.Sequential([\n",
    "    layers.Input(shape=(784,)), # 28x28 input = 784\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(2, activation=None) # 2-dimensional \"output\"\n",
    "])\n",
    "\n",
    "decoder = models.Sequential([\n",
    "    layers.Input(shape=(2,)), # Take in the 2-dimensional output from the encoder\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(784, activation='sigmoid') # Reconstruct the 784-dimensional input\n",
    "])\n",
    "\n",
    "autoencoder = models.Sequential([\n",
    "    encoder,\n",
    "    decoder\n",
    "])\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mae')"
   ],
   "id": "607ebb4f99ae587",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train the autoencoder\n",
    "autoencoder.fit(\n",
    "    mnist.data.values/255, mnist.data.values/255, # input and output are the same - scaled to [0,1]\n",
    "    epochs=10,\n",
    "    batch_size=128\n",
    ")"
   ],
   "id": "953ff74de015d49f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use the encoder to map the data to 2-dimensional space\n",
    "X_autoencoder = encoder.predict(mnist.data.values/255)"
   ],
   "id": "f67a5353ef5a20bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(2,2, figsize=(20,20))\n",
    "\n",
    "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=mnist.target.values, palette='nipy_spectral', ax=ax[0,0])\n",
    "ax[0,0].set_title('2D PCA of MNIST Dataset')\n",
    "ax[0,0].set_xlabel('First Principal Component')\n",
    "ax[0,0].set_ylabel('Second Principal Component')\n",
    "\n",
    "sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=mnist.target.values, palette='nipy_spectral', ax=ax[0,1])\n",
    "ax[0,1].set_title('2D t-SNE of MNIST Dataset')\n",
    "ax[0,1].set_xlabel('First t-SNE Component')\n",
    "ax[0,1].set_ylabel('Second t-SNE Component')\n",
    "\n",
    "sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], hue=mnist.target.values, palette='nipy_spectral', ax=ax[1,0])\n",
    "ax[1,0].set_title('2D UMAP of MNIST Dataset')\n",
    "ax[1,0].set_xlabel('First UMAP Component')\n",
    "ax[1,0].set_ylabel('Second UMAP Component')\n",
    "\n",
    "sns.scatterplot(x=X_autoencoder[:,0], y=X_autoencoder[:,1], hue=mnist.target.values, palette='nipy_spectral', ax=ax[1,1])\n",
    "ax[1,1].set_title('2D Autoencoder of MNIST Dataset')\n",
    "ax[1,1].set_xlabel('First Autoencoder Component')\n",
    "ax[1,1].set_ylabel('Second Autoencoder Component')\n",
    "\n",
    "plt.show()"
   ],
   "id": "59227272ff5743e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There you have it - four different approaches to dimensionality reduction. PCA is the simplest and fastest, but is only able to capture linear relationships between points. t-SNE and UMAP are able to capture non-linear relationships, but are slower than PCA. Autoencoders are also able to capture non-linear relationships, but are generally slower than t-SNE and UMAP.",
   "id": "a6d1c06b3f221426"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
