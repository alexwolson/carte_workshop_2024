{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SF0pPVB4-LHp"
   },
   "source": [
    "# Day 1: Introduction to Machine Learning with Python\n",
    "## Lab 1-2: Decision Trees and Random Forests\n",
    "\n",
    "In the first lab, we used a decision tree classifier to evaluate the quality of different imputation methods, but we didn't really discuss what decision trees are or how they work. In this lab, we will delve into the details of decision trees and explore how they can be used to predict the survival of Titanic passengers."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Install xgboost and graphviz, which we will use later in the lab\n",
    "!pip install -U xgboost graphviz"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7CedHdx-LHw"
   },
   "source": [
    "### Decision Trees\n",
    "\n",
    "Decision trees are popular supervised learning methods used for classification and regression. The tree represents a series of simple decision rules that predict the target when the feature vector is passed through them. Decision trees are easy to understand, can be visualized nicely, require very little data preparation (e.g., we don't need to scale features), and the trained model can be explained easily to others post priori (as opposed to other *black box* methods that are difficult to communicate).\n",
    "\n",
    "###### Example\n",
    "Suppose you wanted to design a simple decision tree for whether (or not) you buy a used car. You might develop something like the following:\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/decision-tree.gif?raw=1\" width=\"500\"/>\n",
    "\n",
    "**YOUR TURN:** Let's say you're browsing Kijiji and come across a used car that: has been road tested, has high mileage, and is a recent year/model.\n",
    "* According to your decision tree model, should you buy this car or not? ____________________________\n",
    "* Will you buy any cars that haven't been road tested (if you follow your model)? ___________________________________\n",
    "\n",
    "Obviously this tree may not be ideal, depending on the situation. For example, you could have a road tested car of a recent year with 2,000,000 km's on it and the model is telling you to buy! (But, you probably shouldn't)\n",
    "\n",
    "### Titanic Dataset\n",
    "\n",
    "Just as in the first lab, we are going to use the Titanic dataset to predict passenger survival. Let's start by loading the data and doing some exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mDl1JKwx-LHx"
   },
   "source": [
    "import pandas as pd # import pandas to get access to dataframe operations\n",
    "from sklearn.datasets import fetch_openml # import function to retrieve relevant datasets\n",
    "\n",
    "full_data = fetch_openml(\"titanic\", version=1, as_frame=True) # Get all data and metadata\n",
    "data = full_data.frame # Extract the relevant data\n",
    "data.survived = pd.to_numeric(data['survived'])\n",
    "data.drop(['boat', 'body', 'home.dest'], axis=1, inplace=True) # Drop irrelevant columns\n",
    "data.head() # view the first 5 rows"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfTLCpGf-LH0"
   },
   "source": [
    "As we have seen already, we can use Pandas to conveniently summarize key aspects of the dataset such as the number of passengers, features, survived/didn't, and their gender. We are also able to identify the number of missing values per feature in the dataset.\n",
    "\n",
    "To accomplish this, we used Pandas flexible indexing capability. The syntax `data[data[col]==val]` allows us to return the subset of rows in `data` where column `col` takes on value `val`. Very powerful!\n",
    "\n",
    "As you may have suspected, the dataset we're using is actually a subset of the total Titanic data. In reality, there were actually 3,547 passengers while the data we're working with only concerns 1309 of them.\n",
    "\n",
    "**YOUR TURN:**\n",
    "Using similar syntax, answer the following questions about the data:\n",
    "* In the dataset, what is the passenger survival rate? ____________________________\n",
    "* How many passengers paid more than $10 for fare? ____________________________\n",
    "* How many passengers had a passenger class (Pclass) of 3? ________________________\n",
    "* With some discussion/exploration and try to determine what features might be the most relevant to passenger survival."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3qH9urfd-LH1"
   },
   "source": [
    "## Your code here\n",
    "##\n",
    "##"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB5y8KEa-LH2"
   },
   "source": [
    "### Preparing the Data\n",
    "\n",
    "Just like in the first lab, we are going to prepare the data for our decision tree model. This involves encoding categorical values as numbers, dropping non-numeric columns, and splitting the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0WjofYtW-LH2"
   },
   "source": [
    "data = data.drop(['name', 'ticket', 'cabin'], axis=1) # remove non-numeric columns\n",
    "\n",
    "data = pd.get_dummies(data, columns=['sex', 'embarked'], drop_first=True) # encode categorical values as numbers\n",
    "\n",
    "data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-a-uiu0-LH3"
   },
   "source": [
    "In the above cell, we dropped the non-numeric categories which are a little harder to deal with. Then we used the `pd.get_dummies` function to replace our categorical variables with numeric ones. Note that in the first lab, we alluded to the fact that we can actually get away with n-1 columns for n categories. This is because a value of False for all n-1 columns implies the nth value is True. While this isn't too important one way or the other for our small toy dataset, you can imagine that for a significantly larger dataset, this could save a lot of memory.\n",
    "\n",
    "##### Model Development\n",
    "\n",
    "OK! Let's get to developing some decision tree models to predict passenger survival. We will start with simple decision trees and develop more complex models from there. Our first step, as in previous labs, is to split our data into a training set and a test set (unseen data). We will then use k-folds cross validation on the training set to try and get the best performing model before finally applying it to the test data.\n",
    "\n",
    "Let's import sklearn's decision tree classifer and split the data (using techniques we covered in the first lab)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uXuxpDkP-LH3"
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "target_data = data[\"survived\"]\n",
    "feature_data = data.iloc[:, data.columns != \"survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_data, target_data, test_size=0.3, random_state=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4U5WwH2-LH4"
   },
   "source": [
    "**YOUR TURN:**\n",
    "* How many samples are in the training set? _______________________\n",
    "* How many samples are in the test set? _______________________\n",
    "* What are the survival rates in each of the datasets? ______________________"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ANVOauwm-LH4"
   },
   "source": [
    "## Your code here\n",
    "##\n",
    "##\n",
    "##"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZp4c8Hg-LH5"
   },
   "source": [
    "##### Dealing with Missing Data: Imputation\n",
    "\n",
    "Before we can fit our decision tree to our training data, we can conduct imputation to replace missing values in our dataset. Previously we did this manually so that we could follow the process, but now we'll use the handy `SimpleImputer` class from sklearn. This class allows us to replace missing values with a specified strategy (e.g., mean, median, most frequent)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fQFGk9hk-LH5"
   },
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "imp.fit(X_train)\n",
    "X_train = imp.transform(X_train) # replace missing data using our imputer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqj-hPeg-LH6"
   },
   "source": "So we've got our data prepared, let's fit a decision tree to our training data."
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VqxAAXVU-LH6"
   },
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_train, clf.predict(X_train))\n",
    "print (\"Accuracy: \", accuracy * 100, \"%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1yMNmyr-LH6"
   },
   "source": [
    "In the above cell, we defined a Decision Tree classifier and fit it to our training set. When we then used it to predict training set values, the resulting accuracy was ~97%.\n",
    "\n",
    "**YOUR TURN:**\n",
    "* Since we are both training and predicting on our training set, why didn't the decision tree achieve 100% accuracy?\n",
    "* What is the performance of this model on the test set?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F4vftzMR-LH7"
   },
   "source": [
    "## Your code here\n",
    "##\n",
    "##\n",
    "##"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4KZ5J-t-LH7"
   },
   "source": [
    "##### Feature Importances\n",
    "\n",
    "One thing we can do is take a look at the relative feature importances of the trained decision tree classifier. This will give us an idea of what the model thinks is more/less important for properly predicting the target.\n",
    "\n",
    "Let's look at the feature importances for a model on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w-1ZcqEN-LH7"
   },
   "source": [
    "for feature, importance in zip(data.columns.drop('survived'), clf.feature_importances_):\n",
    "    print(f'{feature:15}{importance:.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HL_BERf-LH8"
   },
   "source": [
    "As we can see, the tree is placing a higher importance on Sex, Age, and Fare paid. These are interesting observations that we could dig a little deeper into if we wanted to.\n",
    "\n",
    "#### Visualizing the Tree\n",
    "\n",
    "One useful thing we can do is actually visualize our decision tree model! We can use the [graphViz](https://www.graphviz.org/) library to accomplish this:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nQ2xJfJm-LH8"
   },
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz # Package containing visualization tools\n",
    "\n",
    "export_graphviz(clf, out_file=\"mytree.dot\", feature_names=data.columns.drop('survived')) # Save the visualization of the tree\n",
    "with open(\"mytree.dot\") as f: # read the file back in\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph) # display the tree"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EgolEWB-LH8"
   },
   "source": [
    "**YOUR TURN:** Explore the decision tree and answer the following:\n",
    "* What feature does the root node split on?\n",
    "* What is the depth of the decision tree (i.e., the length of the longest path from root to leaf)?\n",
    "* Do you think this decision tree is prone to overfitting? Why/why not?\n",
    "\n",
    "To reduce the degree to which this tree is overfit to the training data, we can force the tree to be of some *maximum depth*. This ensures the tree won't be able to just keep generating new layers to properly classify every sample in the training stage (and, thus, presumably generalize better to the test set).\n",
    "\n",
    "Let's try limiting the max depth to 2 and visualizing the resulting tree."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4uDkSy0a-LH9"
   },
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth = 2)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "export_graphviz(clf, out_file=\"mytree.dot\", feature_names=data.columns.drop('survived'))\n",
    "with open(\"mytree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUJdXnp--LH9"
   },
   "source": [
    "Much simpler! As we can see, our model finds Age, Sex, and Pclass to be the most important features. We would expect this model to have much poorer performance when predicting on the training set (as opposed to our 97% we got above), but perhaps better performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are many hyper-parameters that can be tuned to change how the model performs. Some common parameters that are modified include:\n",
    "1. Max Tree Depth: How \"tall\" do you want your tree to be\n",
    "2. Minimum Samples Per Leaf: This parameter defines the minimum number of training datapoints that fall into a given leaf node in order for that node to be created\n",
    "3. Minimum Samples to Split: This parameter controls the minimum number of samples required to create a decision split\n",
    "\n",
    "To decide the values of each of the parameters, we can use Grid Search combined with cross validation. In Grid Search, we first decide what potential values we want each hyperparameter will take. Then we find every possible combination of parameters and run cross validation on each combination to estimate the performance of that hyperparameter combination.\n",
    "\n",
    "Luckily, `sklearn` has a nice implementation of Grid Search that runs this algorithm for us. Lets see a demo below:"
   ],
   "metadata": {
    "id": "T2gqE8y3_yXP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "clf = tree.DecisionTreeClassifier() # First we define our model without passing in parameters\n",
    "hyperparameter_search = { # Then we decide the possible parameter combinations\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 8, 11],\n",
    "    'min_samples_leaf': [2, 5, 8, 11]\n",
    "} # Since we have 3 parameters with 2 possible values, grid search will test 3^3 combinations\n",
    "evaluation_metric = make_scorer(accuracy_score, # GridSearchCV requires us to wrap our metric function in a \"scorer\"\n",
    "                                greater_is_better = True)\n",
    "\n",
    "grid_search_cv = GridSearchCV(estimator = clf,\n",
    "                              param_grid = hyperparameter_search,\n",
    "                              scoring = evaluation_metric,\n",
    "                              cv = 5) # Set up search algorithm\n",
    "grid_search_cv.fit(X_train, y_train) # Run the search. NOTE: This may take a while\n",
    "\n",
    "print(\"Best Parameters: \", grid_search_cv.best_params_) # Print the parameters\n",
    "print (\"Best CV Accuracy: \", grid_search_cv.best_score_ * 100, \"%\")\n",
    "\n",
    "clf = grid_search_cv.best_estimator_ # Get the best model from the GridSearch\n",
    "accuracy = accuracy_score(y_test, clf.predict(imp.transform(X_test)))\n",
    "print (\"Testing Accuracy: \", accuracy * 100, \"%\") # Print the testing accuracy of the best model"
   ],
   "metadata": {
    "id": "zmpcYAMI_xyX"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the cell above, we tested our two values per hyperparameter and ran grid search to find the best combination from the space we defined. As you may have noticed, the number of combinations tested by Grid Search exponentially increases as you test more values and tune more hyperparameters. This means that performing a grid search is often a task that takes a long period of time and is often note used for more complex models like neural networks."
   ],
   "metadata": {
    "id": "Qrr1hd59EiVK"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yotoft9s-LH9"
   },
   "source": [
    "### Random Forests, Gradient Boosting, Extreme Gradient Boosting\n",
    "\n",
    "Let's (briefly) investigate some more advanced tree models that you have learned about and see if we can improve our performance. We will be using the following models in addition to our decision tree classifier:\n",
    "\n",
    "* Scikitlearn Random Forest classifier\n",
    "* Scikitlearn Gradient boosting classifier\n",
    "* XGBoost classifier\n",
    "\n",
    "Note that XGBoost is similar in theory to Scikitlearn's Gradient boosting classifier. However, XGBoost's implementation is highly efficient than that of Scikitlearn's. Forests are *ensemble* techniques that combine multiple decision trees. As you learned in lecture, and as visualized below, random forest methods usually combine multiple trees through some sort of voting scheme.\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/random-forest.png?raw=1\" width=\"400\"/>\n",
    "\n",
    "In the above example, two trees vote (predict) Class B and another predicts Class A, so the overall ensemble vote goes to the majority (Class B). Boosting, on the other hand, uses multiple trees in in a stage-wise fashion. Popular machine learning software XGBoost has a great explanation for [how this works](https://xgboost.readthedocs.io/en/latest/tutorials/model.html).\n",
    "\n",
    "Let's build some ensemble classifiers!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kJHX6BYO-LH-"
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-yptZhR-LH-"
   },
   "source": [
    "Now, let's see how they perform!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6_8SgflL-LH-"
   },
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf_random = RandomForestClassifier()\n",
    "clf_gradient = GradientBoostingClassifier()\n",
    "clf_xgb = XGBClassifier()\n",
    "\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "scores_random = cross_val_score(clf_random, X_train, y_train, cv=5)\n",
    "scores_gradient = cross_val_score(clf_gradient, X_train, y_train, cv=5)\n",
    "scores_xgb = cross_val_score(clf_xgb, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Decision tree accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(\"Random forest accuracy: %0.2f (+/- %0.2f)\" % (scores_random.mean(), scores_random.std() * 2))\n",
    "print(\"Gradient boosting accuracy: %0.2f (+/- %0.2f)\" % (scores_gradient.mean(), scores_gradient.std() * 2))\n",
    "print(\"XGBoost accuracy: %0.2f (+/- %0.2f)\" % (scores_xgb.mean(), scores_xgb.std() * 2))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HX8UnvyN-LH-"
   },
   "source": [
    "We can see that each of the more sophisticated tree/forest methods improves upon the initial decision tree accuracy in terms of cross-validated accuracy.\n",
    "\n",
    "Let's see how the extreme gradient boosted method performs on the hold-out test set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ItWp3hBx-LH_"
   },
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "imp.fit(X_test)\n",
    "X_test = imp.transform(X_test)\n",
    "\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "accuracy = accuracy_score(y_test, clf_xgb.predict(X_test))\n",
    "\n",
    "print (\"Test set accuracy: \", accuracy * 100, \"%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqpxFuwy-LH_"
   },
   "source": [
    "First, we imputed the missing values in the test set (as we had done for the training set) and then we applied our gradient boosting-based classifier (as trained on the training data). We yielded an 81% accuracy; not bad!\n",
    "\n",
    "**YOUR TURN:**\n",
    "* What features did the gradient boosting algorithm find the most important? __________________\n",
    "* What is the test set accuracy if, instead, you used the Scikitlearn's gradient boosting algorithm? __________________\n",
    "\n",
    "* If you designed a naive classifier that simply guessed 'did not survive' (i.e., Survived = 0) for every row in the test set, how would it perform? ________________________\n",
    "* Is this better or worse than our gradient boosted tree? ____________________________"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "## Your code here",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
